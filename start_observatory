#!/usr/bin/env python3
import os
import sys
import ast
import json
import toml
import yaml
import shutil
import docker
import logging
import argparse
import subprocess
import ansible_runner
from pathlib import Path
from collections import defaultdict


import boto3
from botocore.exceptions import ClientError

logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))
client = docker.from_env()


def create_bucket(bucket_name, region=None):
    """Create an S3 bucket in a specified region

    If a region is not specified, the bucket is created in the S3 default
    region (us-east-1).

    :param bucket_name: Bucket to create
    :param region: String region to create bucket in, e.g., 'us-west-2'
    :return: True if bucket created, else False
    """

    # Create bucket
    try:
        dev = boto3.session.Session(profile_name='vng')
        if region is None:
            s3_client = dev.client('s3')
            s3_client.create_bucket(Bucket=bucket_name)
        else:
            s3_client = dev.client('s3', region_name=region)
            location = {'LocationConstraint': region}
            s3_client.create_bucket(Bucket=bucket_name,
                                    CreateBucketConfiguration=location)
    except ClientError as e:
        logging.error(e)
        return False
    return True


def is_valid_file(parser, filepath):
    """
    Checks the existence of a file and throws an ArgumentParser errror
    if the file does not exist.

    :param parser: the ArgumentParser instance
    :param filepath: the path of the file to check
    :return: the filepath if it exists
    """
    if not os.path.exists(filepath):
        parser.error("The file '{}' does not exist!".format(filepath))
    else:
        return filepath # return an open file handle


def parse_toml_manifest(manifest_filepath):
    """
    Parses the manifest file

    :param manifest_filepath: the path to the toml manifest file
    :return: the dictionaries of the tf configuration files
    """
    # Templates for terraform main.tf.json and variables.tf.json
    # configuration files

    main_tf_json = {
        "module": defaultdict(dict)
    }

    variables_tf_json = {
        "variable": {
            "access_key": {"default": ""},
            "secret_key": {"default": ""},
            "instances_per_az": {"default": 1},
            "availability_zones": {"default": 1},
            "instance_type": {"default": {}},
            "ssh_pub_key": {"default": {}},
            "ssh_private_key": {"default": {}},
            "s3_bucket_name": {"default": {}},
            "region": {},
            "es_domain": {},
            "es_username": {},
            "es_password": {}
        }
    }

    plan = defaultdict()
    docker_settings = defaultdict()
    # Read toml manifest file and populate terraform configuration templates
    with open(args.manifest, "rt") as fin:
        parsed_toml = toml.load(fin)
        try:
            plan["builder"] = parsed_toml["plan"]["builder"]
            plan["app_directory"] = parsed_toml["plan"]["app_directory"]
            if not os.path.isdir(plan["app_directory"]):
                logging.error("The provided app_directory does not exist")
                sys.exit("-1")
            plan["output_directory"] = parsed_toml["plan"]["output_directory"]
            plan["name"] = parsed_toml["plan"]["name"]
            plan["version"] = parsed_toml["plan"]["version"]
            plan["executable"] = parsed_toml["plan"]["executable"]
            plan["command"] = parsed_toml["plan"]["command"]
            docker_settings["hub_username"] = parsed_toml["docker"]["hub_username"]
            docker_settings["hub_password"] = parsed_toml["docker"]["hub_password"]
            access_key = parsed_toml["aws"]["access_key"]
            secret_key = parsed_toml["aws"]["secret_key"]
            metrics_s3_bucket = parsed_toml["aws"]["metrics_s3_bucket"]
            es_username       = parsed_toml["elasticsearch"]["username"]
            es_password       = parsed_toml["elasticsearch"]["password"]
            es_domain         = parsed_toml["elasticsearch"]["domain"]
            
            servers_conf = parsed_toml["aws"]["servers"]
            instance_type = servers_conf["instance_type"]
            ssh_pub_key = servers_conf["ssh_pub_key"]
            ssh_private_key = servers_conf["ssh_private_key"]
            instances_per_az = 1
            availability_zones = 1
            if "instances_per_az" in servers_conf:
                if int(servers_conf["instances_per_az"]) > 5 or int(servers_conf["instances_per_az"]) < 1:
                    logging.warning("You cannot select less than 1 or more than 5 availability zones.")
                    sys.exit(-1)
                instances_per_az = servers_conf["instances_per_az"]
            if "availability_zones" in servers_conf:
                availability_zones = servers_conf["availability_zones"]
        except KeyError as e:
            logging.error("Error: The .toml manifest file does not have the required format")
            logging.error(e)
            sys.exit(-1)
            
        # variables_tf_json["variable"]["access_key"]["default"] = access_key
        # variables_tf_json["variable"]["secret_key"]["default"] = secret_key
        variables_tf_json["variable"]["s3_bucket_name"]["default"] = metrics_s3_bucket
        variables_tf_json["variable"]["instance_type"]["default"] = instance_type
        variables_tf_json["variable"]["ssh_pub_key"]["default"] = ssh_pub_key
        variables_tf_json["variable"]["ssh_private_key"]["default"] = ssh_private_key
        variables_tf_json["variable"]["instances_per_az"]["default"] = instances_per_az
        variables_tf_json["variable"]["availability_zones"]["default"] = availability_zones
        variables_tf_json["variable"]["es_username"]["default"] = es_username
        variables_tf_json["variable"]["es_password"]["default"] = es_password
        variables_tf_json["variable"]["es_domain"]["default"] = es_domain

        for region in servers_conf["regions"]:
            module_name = "observatory-{}".format(region)
            main_tf_json["module"][module_name]["region"] = region
            main_tf_json["module"][module_name]["source"] = "./observatory"
            main_tf_json["module"][module_name]["instance_type"] = servers_conf["instance_type"]
        region = "us-east-1"
        module_name = "es-domain-{}".format(region)
        main_tf_json["module"][module_name]["region"] = region
        main_tf_json["module"][module_name]["source"] = "./elasticsearch"
        
    return main_tf_json, variables_tf_json, plan, docker_settings



def parse_tf_state(tf_state_file):
    """
    Parses the terraform state file and extracts the public IPs of the instances
    and the path to the SSH key which is encoded as instance tag
    
    :param tf_state_file: the path to the terraform state file
    :return: the set of public IPs and the path to the SSH key file
    """
    public_ips = set()
    keypath = False
    result = subprocess.run(['terraform', 'apply', '--auto-approve'], stdout=subprocess.PIPE)
    if result.returncode == 0 and os.path.isfile(tf_state_file):
        # Parse the terraform state to extract the instance details
        with open("terraform.tfstate") as fin:
            tf_state = json.load(fin)
            for resource in tf_state["resources"]:
                if resource["type"] == "aws_instance":
                    for instance in resource["instances"]:
                        public_ip = instance["attributes"]["public_ip"]
                        public_ips.add(public_ip)
                elif resource["type"] == "aws_key_pair":
                    for instance in resource["instances"]:
                        keypath = instance["attributes"]["tags"]["Key"]
    return (public_ips, keypath)


def create_ansible_inventory(public_ips, keypath):
    """
    Creates an ansible inventory for the hosts created by terraform
    
    :param public_ips: the set of public IPs
    :param keypath: the path to the SSH key for the hosts
    :return: True if operation is successful, False otherwise
    """
    try:
        success = False
        # Create the ansible-runner inventory directory
        Path("ansible/inventory").mkdir(parents=True, exist_ok=True)

        # Create the ansible hosts file
        with open("ansible/inventory/hosts", "wt") as fout:
            fout.write("[probes]")
            for ip in public_ips:
                fout.write("\n{} ansible_user=ubuntu ansible_ssh_private_key_file={}".format(ip, keypath))
            success = True
    except OSError as e:
        logging.error("Error while tyring to create the ansible inventory file: {}\n".format(str(e)))
    return success


def create_ansible_extravars(variables_tf_json, image_name):
    """
    Creates the file that stores the Ansible extravar variables
    
    :param variables_tf_json: dictionary with the terraform variables
    :param image_name: the name of the docker image
    :return: True if operation is successful, False otherwise
    """
    success = False
    try:
        # Create the ansible working directory
        Path("ansible/env").mkdir(parents=True, exist_ok=True)
        
        access_key = variables_tf_json["variable"]["access_key"]["default"]
        secret_key = variables_tf_json["variable"]["secret_key"]["default"]
        metrics_s3_bucket = variables_tf_json["variable"]["s3_bucket_name"]["default"]
        container_name = image_name.split(":")[0].split("/")[1]
        
        with open("ansible/env/extravars", "wt") as fout:
            fout.write("---\n")
            fout.write("docker_img: {}\n".format(image_name))
            fout.write("container_name: {}\n".format(container_name))
            fout.write("aws_access_key: {}\n".format(access_key))
            fout.write("aws_secret_key: {}\n".format(secret_key))
            fout.write("bucket_name: {}\n".format(metrics_s3_bucket))
        success = True
        
    except OSError as e:
        logging.error("Error while tyring to create the ansible inventory file: {}\n".format(str(e)))
    except KeyError as e:
        logging.error("Could not find required information to set the measurement variables")
    return success


if __name__ == '__main__':
    from boto3 import Session

    session = Session()
    credentials = session.get_credentials()
    current_credentials = credentials.get_frozen_credentials()


    description = 'Deploy a measurement to the P2P observatory'
    parser = argparse.ArgumentParser(description=description)
    # Add the permitted arguments
    parser.add_argument('-m', '--manifest',
                        type=lambda x: is_valid_file(parser, x),
                        required=True,
                        help="Path to manifest file")

    args = parser.parse_args()

    main_tf_json, variables_tf_json, plan, docker_settings = parse_toml_manifest(args.manifest)
    
    variables_tf_json["variable"]["access_key"]["default"] = current_credentials.access_key
    variables_tf_json["variable"]["secret_key"]["default"] = current_credentials.secret_key
    
    logging.info("Creating requested resources.")
    create_bucket(variables_tf_json["variable"]["s3_bucket_name"]["default"])

    with open("main.tf.json", "wt") as fout:
        json.dump(main_tf_json, fout, indent=4)

    var_tf_file = "./observatory/variables.tf.json"
    with open(var_tf_file, "wt") as fout:
        json.dump(variables_tf_json, fout, indent=4)
    # copy the variables file in the app directory
    var_app_file = os.path.join(plan["app_directory"], os.path.basename(var_tf_file))
    
    shutil.copy(var_tf_file, var_app_file)
    shutil.copy(var_tf_file, "elasticsearch/variables.tf.json")
    
    result = subprocess.run(['terraform', 'init'], stdout=subprocess.PIPE)
    if result.returncode == 0:
        tf_state_file = "terraform.tfstate"
        public_ips, keypath = parse_tf_state(tf_state_file)
        if keypath and len(public_ips) > 0:
            image_name = "{}/{}:{}".format(docker_settings["hub_username"], plan["name"], plan["version"])
            if create_ansible_inventory(public_ips, keypath) and create_ansible_extravars(variables_tf_json, image_name):
                logging.info("Building docker image '{}'".format(image_name))
                script_path = os.path.dirname(os.path.realpath(__file__))
                ansible_path = os.path.join(script_path, "ansible")
                os.chdir(plan["app_directory"])

                with open("plan_executable.sh", "wt") as fout:
                    fout.write("#!/usr/bin/env bash\n")
                    fout.write("./{}\n".format(plan["command"]))
                    fout.write("aws s3 sync output_data_crawls/ s3://$bucketname\n")

                with open("Dockerfile", "wt") as fout:
                    fout.write("FROM vgiotsas/p2p-observatory-base\n")
                    fout.write("LABEL stage=builder\n")
                    fout.write("WORKDIR /app\n")
                    fout.write("ADD . .\n")
                    for builder in plan["builder"]:
                        if builder == "go":
                            fout.write("RUN make build\n")
                        elif builder == "python3":
                            fout.write("RUN python3 -m pip install -r requirements.txt\n")
                    fout.write("WORKDIR /app\n")
                    fout.write("ADD . .\n")
                    fout.write("ENV SHELL /bin/bash\n")
                    fout.write("RUN chmod +x ./{}\n".format(plan["executable"]))
                    fout.write("RUN chmod +x ./plan_executable.sh\n")
                    fout.write("RUN chmod +x ./aws_sync\n")
                    fout.write('CMD ["./plan_executable.sh"]')
                
                try:
                    output = client.images.build(path='/home/ubuntu/ipfs-crawler', tag=image_name, rm=True)
                    output_lines = list()
                    for l in output[1]:
                        output_lines.append(l)
                    
                    img = client.images.get(image_name)
                    assert img.tag(image_name)
                    
                    logging.info("Uploading image to DockerHub")
                    resp = client.login(
                      username=docker_settings["hub_username"],
                      password=docker_settings["hub_password"]
                    )
                    p = client.images.push(os.path.join(image_name), stream=True, decode=True)
                    print(resp)
                    for l in p:
                        output_lines.append(l)
                    # clean-up build
                    client.images.remove(image_name, force=True)
                    logging.info("Deploying measurement to instances")
                    r = ansible_runner.run(private_data_dir=ansible_path, playbook='observatory.yaml')
                    print("{}: {}".format(r.status, r.rc))
                    print("Final status:")
                    print(r.stats)
                except docker.errors.BuildError as e:
                    logging.error("Error while building the docker image:\n{}\n".format(e))
                except docker.errors.APIError as e:
                    logging.error("Error while tyring to push the docker image to DockerHub:\n{}\n".format(e))
        else:
            logging.error("Terraform apply failed.")
    else:
        logging.error("Terraform init failed.")


